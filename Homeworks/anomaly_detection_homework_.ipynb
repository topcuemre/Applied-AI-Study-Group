{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK6xwObDkjDs"
      },
      "source": [
        "# Anomaly Detection Homework\n",
        "\n",
        "This notebook is for anomaly detection homework of Applied AI Week 4. The dataset is given with [this link](https://drive.google.com/file/d/1cZGOZu_zdKLXnH-Ap1w9SMffYXZqa2Ot/view?usp=sharing). If you are having problems with the link, contact with me: safak@inzva.com\n",
        "\n",
        "## Dataset Description\n",
        "\"KDD CUP 99 data set is used mainly to analyze the different\n",
        "attacks. It consists of nearly 4,900,000 samples with 41\n",
        "features and each sample is classified as either normal or\n",
        "attack\" [explanation from this source](https://www.ripublication.com/ijaer18/ijaerv13n7_81.pdf)\n",
        "\n",
        "## Task Description\n",
        "\n",
        "The dataset is prepared and preprocessed for anomaly detection task, the dataset contains \"Probe\" and \"Normal\" targets. \"Probe\" is anomaly, \"Normal\" is normal. \n",
        "\n",
        "**You are supposed to build a anomaly detection model** with **Vanilla Autoencoder**, **Variational Autoencoder** and **Denoising Autoencoder**. However you are not restricted by autoencoer, you can implement a fancy state-of-the-art ensemble 1000B parameter model. It is really up to you. \n",
        "\n",
        "We don't really want you to do sloppy homework.\n",
        "\n",
        "The variable descriptions:\n",
        "\n",
        "- train set: kdd_train_probe\n",
        "- validation set (for hyperparam tuning): kdd_valid_probe\n",
        "- test set: kdd_test_v2_probe\n",
        "\n",
        "## What will you report?\n",
        "Report your average macro f1 score on test set:\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
        "print(f1)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIZsN_bUmDaD"
      },
      "source": [
        "# Preparation (do not edit this part)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdVUhQtGK44s",
        "outputId": "20668e63-977c-4f98-92c4-bc0a062962c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0foq7wEgm3hD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import sys\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BHWVxbClBdhl"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 10, 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IjeIew14vgeQ"
      },
      "outputs": [],
      "source": [
        "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
        "\n",
        "kdd = pd.read_csv('/content/drive/MyDrive/kdd.csv')\n",
        "kdd = kdd.iloc[:,1:43]\n",
        "kdd = kdd.drop(['Protocol Type', 'Service', 'Flag'], axis = 1)\n",
        "\n",
        "kdd_train = kdd.iloc[0:102563, :]\n",
        "kdd_test = kdd.iloc[102563:183737, :]\n",
        "\n",
        "kdd_train_probe = kdd_train[(kdd_train.Type_Groups == 'Normal') | (kdd_train.Type_Groups == 'Probe')]\n",
        "kdd_test_probe = kdd_test[(kdd_test.Type_Groups == 'Normal') | (kdd_test.Type_Groups == 'Probe')]\n",
        "\n",
        "kdd_train_probe['Type_Groups'] = np.where(kdd_train_probe['Type_Groups'] == 'Normal', 0, 1)\n",
        "kdd_test_probe['Type_Groups'] = np.where(kdd_test_probe['Type_Groups'] == 'Normal', 0, 1)\n",
        "\n",
        "kdd_valid_probe = kdd_test_probe.iloc[14000:34000,:]\n",
        "kdd_test_v2_probe = pd.concat([kdd_test_probe.iloc[0:14000,:], kdd_test_probe.iloc[34001:64759,:]])\n",
        "\n",
        "\n",
        "# classify anomalies and normals\n",
        "# train set: kdd_train_probe\n",
        "# validation set (for hyperparam tuning): kdd_valid_probe\n",
        "# test set: kdd_test_v2_probe\n",
        "# avg. macro f1 score on test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcxDLPSSmIPd"
      },
      "source": [
        "## Pytorch DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O7bI7a2t2J4R"
      },
      "outputs": [],
      "source": [
        "# NORMAL: class label 0\n",
        "# ANOMALY: class label 1\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        super(TabularDataset, self).__init__()\n",
        "        self.df = df\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.df.iloc[idx, :-1].to_numpy()\n",
        "        return {\n",
        "            \"samples\": torch.Tensor(data)\n",
        "        }\n",
        "    \n",
        "class TabularDatasetTest(Dataset):\n",
        "    def __init__(self, df):\n",
        "        super(TabularDatasetTest, self).__init__()\n",
        "        self.df = df\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.df.iloc[idx, :-1].to_numpy()\n",
        "        label = self.df.iloc[idx, -1]\n",
        "        return {\n",
        "            \"samples\": torch.Tensor(data),\n",
        "            \"labels\": torch.tensor(label)\n",
        "        }\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_normal = kdd_train_probe[kdd_train_probe.Type_Groups == 0]\n",
        "val_normal = kdd_valid_probe[kdd_valid_probe.Type_Groups == 0]\n",
        "test_normal = kdd_test_v2_probe[kdd_test_v2_probe.Type_Groups == 0]\n",
        "\n",
        "\n",
        "train_data = TabularDataset(train_normal)\n",
        "val_data = TabularDataset(val_normal)\n",
        "test_data_all = TabularDatasetTest(kdd_test_v2_probe)\n",
        "\n",
        "# train_dataloader: For training autoencoder. Contains only normal samples\n",
        "# val_dataloader: For evaluating autoencoder at training phase.\n",
        "#                 then use it for tune the threshold value.\n",
        "#                 N.B: setting batch size of 1 at threshold finding phase should be more reasonable:\n",
        "#                 DataLoader(val_data, shuffle = False, batch_size = 1)\n",
        "#\n",
        "# test_all_dataloader: Contains all test samples (anomalies and normals). Use it for\n",
        "#                      calculating your metrics\n",
        "\n",
        "# N.B.: finding a threshold value is challenging. iterating all val_dataloader and calculating\n",
        "#       metrics over it works but it is expensive computationally.\n",
        "\n",
        "train_dataloader = DataLoader(train_data, shuffle = True, batch_size = BATCH_SIZE)\n",
        "val_dataloader = DataLoader(val_data, shuffle = False, batch_size = BATCH_SIZE)\n",
        "test_all_dataloader = DataLoader(test_data_all, shuffle = False, batch_size = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPMxDRei_bpI"
      },
      "source": [
        "# VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6h9mMOzXwVdR"
      },
      "outputs": [],
      "source": [
        "# VAE implementation in PyTorch\n",
        "\n",
        "class LinearVAE(nn.Module):\n",
        "    def __init__(self, n_features, latent_dim):\n",
        "        super(LinearVAE, self).__init__()\n",
        "        self.n_features = n_features\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(n_features, 20),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.encoder2mean = nn.Linear(20, latent_dim)\n",
        "        self.encoder2logvar = nn.Linear(20, latent_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, n_features)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        bs = x.size(0)\n",
        "        out = self.encoder(x)\n",
        "        mu = self.encoder2mean(out)\n",
        "        log_var = self.encoder2logvar(out)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        out = self.decoder(z)\n",
        "        return out, mu, log_var\n",
        "        \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        sample = mu + (eps * std)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss(recon_x, x, mu, log_var, criterion):\n",
        "    variational_beta = 1\n",
        "    recon_loss = criterion(recon_x, x)\n",
        "    kldivergence = (-0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())) / x.size(0)\n",
        "    return recon_loss + variational_beta * kldivergence"
      ],
      "metadata": {
        "id": "T_c6UpbbrAK0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Efh0YtnP1zSJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, criterion, val_dataloader,device):\n",
        "    total = len(val_dataloader)\n",
        "    loss_a = []\n",
        "    val_loss = 0\n",
        "    val_batch_loss = 0\n",
        "    val_num_batches = 0\n",
        "    for step, batch in enumerate(val_dataloader):\n",
        "      model.eval()\n",
        "      val_batch_loss += 1\n",
        "      batch, = tuple(t.to(device) for t in batch.values())\n",
        "\n",
        "      with torch.no_grad():\n",
        "          out, mu, log_var = model.forward(batch)\n",
        "      \n",
        "      loss = vae_loss(out, batch, mu, log_var, criterion)\n",
        "\n",
        "      loss_a.append(loss.detach().cpu().numpy())\n",
        "    return np.mean(loss_a)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, train_dataloader, val_dataloader, device, num_epochs):\n",
        "    model = model.to(device)\n",
        "    total = len(train_dataloader) * num_epochs\n",
        "    best_loss = 0\n",
        "    print('Training ...')\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        batch_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            model.train()\n",
        "            batch, = tuple(t.to(device) for t in batch.values())\n",
        "            optimizer.zero_grad()\n",
        "            out, mu, log_var = model.forward(batch)\n",
        "            loss = vae_loss(out, batch, mu, log_var, criterion)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "            num_batches +=1\n",
        "\n",
        "        val_loss = evaluate(model, criterion, val_dataloader, device)\n",
        "        print('\\n')\n",
        "        print(f\"{epoch+1}/{num_epochs}\")\n",
        "        print(f\"Training loss: {batch_loss / num_batches}, Validation loss: {val_loss}\")\n",
        "\n",
        "        if val_loss > best_loss:\n",
        "          print(f'----Best Model Saved----')\n",
        "          best_loss = val_loss\n",
        "          best_model = model\n",
        "\n",
        "    return model, best_model\n"
      ],
      "metadata": {
        "id": "xICXQapupcqb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LinearVAE(train_data.df.shape[1] - 1, 10)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=1e-5)\n",
        "criterion = torch.nn.L1Loss(reduction=\"sum\")\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "lPguW8pxTlXz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, best_model = train(\n",
        "    model, \n",
        "    optimizer,\n",
        "    criterion, \n",
        "    train_dataloader, \n",
        "    val_dataloader, \n",
        "    device, \n",
        "    epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpX0aWQ5PRm7",
        "outputId": "e642ed88-0cb7-41fd-a882-6c83a2975ee4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ...\n",
            "\n",
            "\n",
            "1/20\n",
            "Training loss: 745.1585603011282, Validation loss: 515.259033203125\n",
            "----Best Model Saved----\n",
            "\n",
            "\n",
            "2/20\n",
            "Training loss: 451.58684136240106, Validation loss: 510.6116027832031\n",
            "\n",
            "\n",
            "3/20\n",
            "Training loss: 350.5602393702457, Validation loss: 490.1463623046875\n",
            "\n",
            "\n",
            "4/20\n",
            "Training loss: 333.1674997229325, Validation loss: 483.7287292480469\n",
            "\n",
            "\n",
            "5/20\n",
            "Training loss: 316.4903158288253, Validation loss: 448.7687072753906\n",
            "\n",
            "\n",
            "6/20\n",
            "Training loss: 294.5082332209537, Validation loss: 444.7658996582031\n",
            "\n",
            "\n",
            "7/20\n",
            "Training loss: 288.96093521118166, Validation loss: 444.7655334472656\n",
            "\n",
            "\n",
            "8/20\n",
            "Training loss: 285.267223117226, Validation loss: 441.2275390625\n",
            "\n",
            "\n",
            "9/20\n",
            "Training loss: 282.0441728893079, Validation loss: 440.9853210449219\n",
            "\n",
            "\n",
            "10/20\n",
            "Training loss: 279.0639706059506, Validation loss: 436.9300231933594\n",
            "\n",
            "\n",
            "11/20\n",
            "Training loss: 277.03750333284074, Validation loss: 432.9464416503906\n",
            "\n",
            "\n",
            "12/20\n",
            "Training loss: 275.57396432976975, Validation loss: 435.4291687011719\n",
            "\n",
            "\n",
            "13/20\n",
            "Training loss: 274.01186535483913, Validation loss: 433.0887756347656\n",
            "\n",
            "\n",
            "14/20\n",
            "Training loss: 272.65636193124874, Validation loss: 433.1953125\n",
            "\n",
            "\n",
            "15/20\n",
            "Training loss: 271.56899420085705, Validation loss: 433.2317810058594\n",
            "\n",
            "\n",
            "16/20\n",
            "Training loss: 270.8469683396189, Validation loss: 432.57391357421875\n",
            "\n",
            "\n",
            "17/20\n",
            "Training loss: 269.68863210176164, Validation loss: 432.1125183105469\n",
            "\n",
            "\n",
            "18/20\n",
            "Training loss: 268.96942981920745, Validation loss: 431.10260009765625\n",
            "\n",
            "\n",
            "19/20\n",
            "Training loss: 268.4969806068822, Validation loss: 431.837890625\n",
            "\n",
            "\n",
            "20/20\n",
            "Training loss: 267.8655601903012, Validation loss: 429.2256774902344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLgaxnaR_eEp"
      },
      "source": [
        "# Vanilla AE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9gwWry8U-3Zs"
      },
      "outputs": [],
      "source": [
        "class VanillaAE(nn.Module):\n",
        "    def __init__(self, n_features, latent_dim):\n",
        "        super(VanillaAE, self).__init__()\n",
        "        self.n_features = n_features\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(n_features, 20),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(20, latent_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, n_features)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        bs = x.size(0)\n",
        "        out = self.encoder(x)\n",
        "        out = self.decoder(out)\n",
        "        return out"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "anomaly_detection_homework.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}