{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 0: Introduction to Deep Learning Frameworks\n",
    "\n",
    "## Notebook 1: MNIST Classification with a Dense Neural Network on Tensorflow2\n",
    "\n",
    "Welcome to Applied AI study group! As a starter pack to the study group, you have three notebooks to get acquainted with the commonly used deep learning frameworks.\n",
    "\n",
    "We will use Python together with Jupyter to go through all our notebooks. You can use the Python3 installed and available on your system, or you can go to the [python website](https://www.python.org/downloads/) to install Python3 to your system. Another alternative is to use [miniconda](https://docs.conda.io/en/latest/miniconda.html) to install python from scratch along with some useful packages.\n",
    "\n",
    "## 0. Problem Definition\n",
    "\n",
    "In all of the notebooks of this preparatory week, the problem we are trying to solve is **classification** using machine learning. More specifically, we have images and different categories. We are going to build models that will predict the category of a given image.\n",
    "\n",
    "The dataset we are using in this notebook is [MNIST](http://yann.lecun.com/exdb/mnist/). This is a widely used classification dataset in computer vision and machine learning fields, consisting of handwritten digits from zero to nine. We will try to train a model that predicts the digit given an image.\n",
    "\n",
    "## 1. Installation\n",
    "\n",
    "To install Jupyter notebook on your system, you can run the following command to install with pip:\n",
    "\n",
    "    pip install notebook\n",
    "    \n",
    "Or if you are using conda, you can run:\n",
    "\n",
    "    conda install -c conda-forge notebook\n",
    "    \n",
    "Go to the directory where these notebooks are contained and run:\n",
    "\n",
    "    jupyter notebook\n",
    "    \n",
    "to open up your notebooks and begin your adventure!\n",
    "\n",
    "## 2. Imports and Checks\n",
    "\n",
    "In this notebook, we are starting our journey of deep learning frameworks with [TensorFlow2](https://www.tensorflow.org). We first install TensorFlow2 using the official guidelines found [here](https://www.tensorflow.org/install).\n",
    "\n",
    "The whole process usually boils down to running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the installation is done, let's import tensorflow first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the version and make sure that we are using the right version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are seeing any version above >= 2.0.0, then you are good to go.\n",
    "\n",
    "Below we are installing and importing a high level wrapper around `tf.data` named `tensorflow_datasets` to directly load datasets that are ready to be trained! We will only use this package to show the list of datasets available within `tf.data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the necessary libraries for data exploration and some further data operations. If any of these packages are not installed on your system, please install them via `pip` or `conda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "We will use `tensorflow_datasets` to load the MNIST dataset. MNIST may be the most commonly used dataset in computer vision because of its simplicity. We are splitting the data as *train* and *test* sets and we are not using batching yet, so the `batch_size` parameter is -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_training, mnist_test = tfds.load('mnist', split=['train', 'test'], batch_size=-1, as_supervised=True)                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see a summary of the pixel values of the MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mnist_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the images and labels separately to prepare for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_training_images, mnist_training_labels = mnist_training[0], mnist_training[1]\n",
    "mnist_test_images, mnist_test_labels = mnist_test[0], mnist_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, we print the shapes. MNIST contains $28 \\times 28$ grayscale images. In addition, we have 60,000 training images and 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_training_images.shape)\n",
    "print(mnist_training_labels.shape)\n",
    "\n",
    "print(mnist_test_images.shape)\n",
    "print(mnist_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the first training image using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(mnist_training_images[0][:, :, 0] ,cmap = 'gray')\n",
    "print(mnist_training_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize the first test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(mnist_test_images[0][:, :, 0] ,cmap = 'gray')\n",
    "print(mnist_test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we begin data preprocessing. We will use `tf.reshape` to change the shapes of the images into trainable vectors of size 784 (28 x 28). First we get the shapes separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_images = mnist_training_images.shape[0]\n",
    "num_test_images = mnist_test_images.shape[0]\n",
    "\n",
    "img_width, img_height = mnist_training_images.shape[1], mnist_training_images.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a dense network, we *flatten* the images into vectors of $784 \\times 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_training_images = tf.reshape(mnist_training_images, shape=(num_training_images, img_width * img_height))\n",
    "mnist_test_images = tf.reshape(mnist_test_images, shape=(num_test_images, img_width * img_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we actually changed the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_training_images.shape)\n",
    "print(mnist_test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another preprocessing step is to normalize the data. As you already know from studying deep learning, normalization is a key step on preparing the data. MNIST pixels are normally between 0 and 255. We normalize the images by dividing each pixel to 255 to map the pixel values between 0 and 1.\n",
    "\n",
    "Let's first look at the minimum and maximum values of the pixels and the labels.\n",
    "\n",
    "Please note that we do not have to normalize the labels. However, we need to create one-hot vectors from label values. More on that in a short while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(mnist_training_images[0]),np.amin(mnist_training_images[0]))\n",
    "print(np.amax(mnist_test_images[0]),np.amin(mnist_test_images[0]))\n",
    "\n",
    "print(np.amax(mnist_training_labels),np.amin(mnist_training_labels))\n",
    "print(np.amax(mnist_test_labels),np.amin(mnist_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide all the pixel values to 255.0 and cast them to type `tf.float32`. We also cast the label values into `tf.int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    y = tf.cast(y, tf.int64)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create one-hot vectors for the labels for the neural network to calculate the error. Below, we are creating the one-hot vectors and actually creating the dataset with the batch size 128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(xs, ys, n_classes=10):\n",
    "    ys = tf.one_hot(ys, depth=n_classes)\n",
    "    \n",
    "    return tf.data.Dataset.from_tensor_slices((xs, ys)) \\\n",
    "    .map(preprocess) \\\n",
    "    .shuffle(len(ys)) \\\n",
    "    .batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(mnist_training_images, mnist_training_labels)\n",
    "test_dataset = create_dataset(mnist_test_images, mnist_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "train_dataset.element_spec    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! We have our dataset now. Let's check the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_labels = next(iter(train_dataset))\n",
    "print(batch_images.shape)\n",
    "print(batch_labels.shape)\n",
    "print(np.amax(batch_images[0]),np.amin(batch_images[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data loader works like a charm. We have 128 vectors that are 784 dimensional as images, and 128 vectors that are 10 dimensional as labels.\n",
    "\n",
    "Our maximum pixel value is 1 and the minimum is 0. Data is ready!\n",
    "\n",
    "Let's visualize the first image in our batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tf.reshape(batch_images[0], shape=(img_width, img_height, 1))[:, :, 0] ,cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Creation\n",
    "\n",
    "Let's define the hyperparameters of the model that we are going to use. We will create a three layer neural network consisting of dense layers. The `layer_neurons` variable below defines the sizes of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 784\n",
    "label_shape = 10\n",
    "\n",
    "lr = 0.003\n",
    "\n",
    "layer_neurons = [\n",
    "    [input_shape, 200],\n",
    "    [200, 80],\n",
    "    [80, label_shape],\n",
    "]\n",
    "\n",
    "bias_shapes = [200, 80, label_shape]\n",
    "initializer = tf.initializers.glorot_uniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a function that creates a dense layer in TF2. It is simply multiplying the inputs and the weights, adds biases and passes the whole calculation from a sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(inputs, weights, bias):\n",
    "    return tf.nn.sigmoid(tf.matmul(inputs, weights) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write functions to initialize the weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(shape, name):\n",
    "    return tf.Variable(initializer(shape), name=name, trainable=True, dtype=tf.float32)\n",
    "\n",
    "def get_bias(shape, name):\n",
    "    return tf.Variable(initializer([shape]), name=name, trainable=True, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the weights and biases to be used in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "bias = []\n",
    "i = 0\n",
    "for layer in layer_neurons:\n",
    "    weights.append(get_weight(layer, 'weight{}'.format(i)))\n",
    "    i+=1\n",
    "\n",
    "i = 0\n",
    "for layer in bias_shapes:\n",
    "    bias.append(get_bias(layer, 'bias{}'.format(i)))\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an important step, we define the function that creates our whole neural network. As mentioned earlier, we have a three layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    l1 = dense_layer(input, weights[0], bias[0])\n",
    "    l2 = dense_layer(l1, weights[1], bias[1])\n",
    "    l3 = dense_layer(l2, weights[2], bias[2])\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the optimizer and the loss function.\n",
    "\n",
    "One thing to note here is that since we are using `softmax_cross_entropy_with_logits` as the loss function, we don't have to include a `softmax` layer into our model. The reason for this is that the `softmax_cross_entropy_with_logits` function already applies a softmax to the given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(lr)\n",
    "\n",
    "def loss(pred, target):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(target, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define one training step below. Note that we are using `tf.GradientTape` here for automatic differentiation. Therefore, we don't have to define the backward pass operations while creating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, outputs, epoch):\n",
    "    epoch_loss_avg = None\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss(model(inputs), outputs)\n",
    "        grads = tape.gradient(current_loss, weights)\n",
    "        optimizer.apply_gradients(zip(grads, weights))\n",
    "    \n",
    "    epoch_loss_avg = tf.reduce_mean(current_loss)\n",
    "    \n",
    "    return epoch_loss_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Below we train our model for 10 epochs. We traverse over all training dataset.\n",
    "\n",
    "Total loss is divided by number of iterations to get average loss for each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    i = 0\n",
    "    for train_data in train_dataset:\n",
    "        batch_images, batch_labels = train_data\n",
    "        iter_loss = train_step(model, batch_images, batch_labels, epoch)\n",
    "        epoch_loss += iter_loss\n",
    "        i+=1\n",
    "    print(\"--- On epoch {} ---\".format(epoch))\n",
    "    tf.print(\"| Loss: \", epoch_loss/i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "We use the trained model over the test dataset and normalize with number of test samples to obtain the final accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0 \n",
    "for test_data in test_dataset:\n",
    "    batch_images, batch_labels = test_data\n",
    "    predictions = model(batch_images)\n",
    "    predictions = tf.nn.softmax(predictions)\n",
    "    equality = tf.math.equal(np.argmax(predictions, axis=1), np.argmax(batch_labels, axis=1))\n",
    "    acc += np.sum(equality)\n",
    "acc /= 10000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing this notebook! You can move on to the next one, which we are going to use PyTorch to classify MNIST examples.\n",
    "\n",
    "**Bonus - Try to:**\n",
    "\n",
    "- Get a test image\n",
    "- Plot the image\n",
    "- Make a model prediction on the image\n",
    "- Print the predicted label and the actual label!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
