{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2289cb06",
   "metadata": {},
   "source": [
    "# Week 0: Introduction to Deep Learning Frameworks\n",
    "\n",
    "## Notebook 2: MNIST Classification with a Convolutional Neural Network on PyTorch\n",
    "\n",
    "Welcome to the second notebook of deep learning frameworks week! In this notebook, we are going to build a convolutional neural network in PyTorch to classify MNIST images. Objective of this week is to get you acquainted with PyTorch basics.\n",
    "\n",
    "## 0. Problem Definition\n",
    "\n",
    "In this notebook, once again we are classifying handwritten digits with the MNIST dataset! This time, however, we are going to be using a convolutional neural network (CNN) instead of a fully connected one as in the previous notebook.\n",
    "\n",
    "Let's begin by installing PyTorch:\n",
    "\n",
    "## 1. Install PyTorch\n",
    "\n",
    "Follow the [official guidelines](https://pytorch.org/get-started/locally/) to install PyTorch.\n",
    "\n",
    "You can find the updated instructions for the latest versions there. If you wish to install an older version, you can also install using the instructions on the website.\n",
    "\n",
    "## 2. Imports\n",
    "\n",
    "Let's start by importing the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82685e9",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "We are going to directly use `torchvision.datasets` to quickly obtain the MNIST data. This packages contains many datasets that are ready to use. Feel free to go through its documentation.\n",
    "\n",
    "First we define the necessary transformations as the preprocess steps. Namely, we convert the images to tensors and normalize them with mean 0.1307 and standard deviation 0.3081. These values are known for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4300bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a6a5e",
   "metadata": {},
   "source": [
    "Let's get the MNIST data by applying our transforms. Setting `download=True` will download the dataset if it's not downloaded already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc88c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e226ed0",
   "metadata": {},
   "source": [
    "Below we prepare the data loaders to be used in training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=128)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acc06a",
   "metadata": {},
   "source": [
    "## 4. Model Creation\n",
    "\n",
    "Now let's define our convolution neural network using an object oriented approach. Custom networks in PyTorch are designed as classes derived from `nn.Module`. Below we see an example of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b16591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ea4f6",
   "metadata": {},
   "source": [
    "In PyTorch, we only write the `__init__` method and the `forward` method. We do not have to write the backward pass operations since PyTorch supports automatic differentiation.\n",
    "\n",
    "Our model contains two convolutional layers followed by a max pooling operation, then two fully connected layers with dropouts. If you look at the forward function, you can clearly see the sequential operations applied in the neural network.\n",
    "\n",
    "Next up, we create our model, optimizer, and scheduler. Optimizer starts with learning rate 1.0 and after each epoch, decreases the learning rate with a factor of 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164eb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180f438",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate\n",
    "\n",
    "Below we train our model for 14 epochs and evaluate the accuracy on the test set after each epoch. Notice how we change the mode of the model with `model.train()` and `model.eval()`. We use the train dataloader for training and test dataloader for testing.\n",
    "\n",
    "One thing to note is that using `F.nll_loss()` makes the model work with negative log-likelihood loss. Therefore, we applied `log_softmax` at the end of our convolutional neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775f3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_interval = 10\n",
    "num_epochs = 14\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    \n",
    "    # Evaluate after the epoch\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427a54f",
   "metadata": {},
   "source": [
    "Congratulations on finishing this notebook. In our next notebook, we will be looking at our final framework Keras, and we will use it to classify Cifar-10 images.\n",
    "\n",
    "**Bonus - Try to:**\n",
    "\n",
    "- Get a test image\n",
    "- Plot the image\n",
    "- Make a model prediction on the image\n",
    "- Print the predicted label and the actual label!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
