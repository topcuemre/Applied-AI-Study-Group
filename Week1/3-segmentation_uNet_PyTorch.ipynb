{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Computer Vision\n",
    "\n",
    "## Notebook 3: Semantic Segmentation with a U-Net Convolutional Neural Network using PyTorch\n",
    "\n",
    "Welcome to the fourth notebook of this week's Applied AI Study Group! We will study semantic segmentation problem with MSRC-v2 image dataset provided by Microsoft. The aim of our task will be to make object segmentation in the given images.\n",
    "\n",
    "### 1. Semantic Segmentation\n",
    "\n",
    "Semantic Segmentation aims to label each pixel (aka classify pixel-wise) of a given image. We treat different objects of the same class as they are same object. On contrast, instance segmentation treats each objects of the same class as they are different objects, hence, label them differently such as object 1, object 2, etc. In this notebook, we will tackle the problem of semantic segmentation. The pixel-wise operations can be applied via segmentation on images, for example, portrait mode in images requires to differentiate between foreground and background of an image. We blur out the pixels which are classified as background. \n",
    "\n",
    "So, how do we build our model for this case? We know that the capabilities of convolution filters are proven in terms of their capabilities in processing structured data such as images. However, they reduce the size of their input vector depends on their kernel size. We need a model that outputs the same size of input vector since we want to retrieve the same image we give into the model. Luckily for us, we have U-Net Architecture for these kind of tasks. We will study U-Nets in the following section.\n",
    "\n",
    "### 2. U-Net Convolutional Neural Network\n",
    "\n",
    "![U-Net Model Architecture](./images/u-net.jpg \"Example U-Net\")\n",
    "\n",
    "We can divide the U-Net architecture into two parts while studying it: the left half of the network is responsible for information encoding and the right half of the network is responsible for information decoding.\n",
    "\n",
    "* Encoder captures the context of image with a series of convolution and pooling operations, hence, it is responsible for feature extraction from the input image. \n",
    "* The middle layer between the encoder and the decoder is called bottlenecek representation of the input image. It is the high level feature representation of input data, e.g. objects and events. Using the information retrieved from each layer, we re-construct an output image with a same size as input image. For that, we utilize decoder of U-Net.\n",
    "* Decoder upsamples the bottleneck representation to recover spatial locations (aka pixel-wise information) for assigning class labels to each pixel of the input image. For that, we recover the same size output image as the input image. During upsampling, we add extracted features from encoder part as in skip connections. We cannot afford to lose much information during encoding-decoding. Therefore, we need low-level features such as edges to have better results in classifying pixels.\n",
    "\n",
    "### 3. Imports and Checks\n",
    "\n",
    "You should have installed Numpy and Matplotlib using `pip` and, PyTorch using [Week 0 - Notebook 2](https://github.com/inzva/Applied-AI-Study-Group/blob/add-frameworks-week/Applied%20AI%20Study%20Group%20%236%20-%20January%202022/Week%200/2-mnist_classification_convnet_pytorch.ipynb).\n",
    "\n",
    "The python file segmentation_dataset.py and MSRC-v2 image dataset can be found in the following link: [Segmentation](https://drive.google.com/drive/u/1/folders/18bKQKwvjuXbjNDMI-ktRD6tet9tcxNkL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from datasets.segmentation_dataset import SegmentationData, label_img_to_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following two code cells are running successfully, then, you are good to go further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the data using the code provided in segmentation_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join('./datasets','segmentation')\n",
    "\n",
    "train_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/train.txt')\n",
    "val_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/val.txt')\n",
    "test_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first print calls are for double-checking the data loading.\n",
    "\n",
    "Then, we visualize a couple of example images to observe what our input images look like and what our output images should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Img size: \", train_data[0][0].size())\n",
    "print(\"Segmentation size: \", train_data[0][1].size())\n",
    "\n",
    "num_example_imgs = 4\n",
    "plt.figure(figsize=(10, 5 * num_example_imgs))\n",
    "for i, (img, target) in enumerate(train_data[:num_example_imgs]):\n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 1)\n",
    "    plt.imshow(img.numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 2)\n",
    "    plt.imshow(label_img_to_rgb(target.numpy()))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build our model in the following cell. Since semantic segmentation is a challenging task, we will use a pretrained model from torchvision for the encoder of our U-Net architecture. Then, we will build our decoder using Upsample, ConvTranspose2d, and LeakyReLU activation layers on top of MobileNet encoder.\n",
    "\n",
    "One important note for pretrained model: We use [MobileNet](https://arxiv.org/pdf/1704.04861.pdf) for its fast inference time. We don't need a more complex model with much inference time for the purpose of this notebook. You can check the models provided by [torchvision](https://pytorch.org/vision/0.8/models.html) for trying out different models. The MobileNet is trained with [ImageNet dataset](https://www.image-net.org/index.php). It is trained for classification task.  Hence, you can see that we exclude the last layer of MobileNet because we don't need the classifier layer but we need the rest of the network for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class SegmentationNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=23, hparams=None):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        mobile_network = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "        layers = list(mobile_network.children())[:-1]  # 1x1280x8x8\n",
    "        layers.append(nn.Conv2d(1280, 120, 1, 1))  # 1x160x8x8\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.Upsample(scale_factor=4))  # 1x160x32x32\n",
    "        layers.append(nn.ConvTranspose2d(120, 80, 3, 2))  # 1x120x64x64\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.ConvTranspose2d(80, 60, 9, dilation=2))  # 1x80x80x80\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.ConvTranspose2d(60, 40, 9, dilation=2))  # 1x60x96x96\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.ConvTranspose2d(40, 40, 11, dilation=2))  # 1x40x116x116\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        layers.append(nn.Upsample(scale_factor=2))  # 1x40x232x232\n",
    "        layers.append(nn.ConvTranspose2d(40, 23, 7))  # 1x23x240x240\n",
    "        layers.append(nn.LeakyReLU(0.1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.network(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify our training hyperparameters and set the rest of the adjustments for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"lr\" : 0.001,\n",
    "    \"batch_size\" : 4,\n",
    "    \"num_epochs\" : 4\n",
    "}  \n",
    "\n",
    "model = SegmentationNN(hparams=hparams)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"lr\"])\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=hparams[\"batch_size\"], shuffle=True)\n",
    "print(train_loader)\n",
    "print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a test run for training loop to check if anything is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (inputs, targets) in train_data[0:4]:\n",
    "    inputs, targets = inputs, targets\n",
    "    outputs = model(inputs.unsqueeze(0).to(device))\n",
    "    losses = criterion(outputs, targets.unsqueeze(0).to(device))\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train our model using the classical PyTorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training starts!')\n",
    "for epoch in range(hparams[\"num_epochs\"]):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print(\"Epoch: %d Loss: %.3f\" % (epoch + 1, epoch_loss / 276))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add validation into training loop for per epoch\n",
    "#TODO: add test code and visualize some results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
