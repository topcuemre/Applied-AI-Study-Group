{"cells":[{"cell_type":"markdown","metadata":{"id":"RK6xwObDkjDs"},"source":["# Anomaly Detection Homework\n","\n","This notebook is for anomaly detection homework of Applied AI Week 2. The dataset is given with [this link](https://drive.google.com/file/d/1cZGOZu_zdKLXnH-Ap1w9SMffYXZqa2Ot/view?usp=sharing). If you are having problems with the link, contact with me: safak@inzva.com\n","\n","## Dataset Description\n","\"KDD CUP 99 data set is used mainly to analyze the different\n","attacks. It consists of nearly 4,900,000 samples with 41\n","features and each sample is classified as either normal or\n","attack\" [explanation from this source](https://www.ripublication.com/ijaer18/ijaerv13n7_81.pdf)\n","\n","## Task Description\n","\n","The dataset is prepared and preprocessed for anomaly detection task, the dataset contains \"Probe\" and \"Normal\" targets. \"Probe\" is anomaly, \"Normal\" is normal. \n","\n","**You are supposed to build a anomaly detection model** with **Vanilla Autoencoder**, **Variational Autoencoder** and **Denoising Autoencoder**. However you are not restricted by autoencoer, you can implement a fancy state-of-the-art ensemble 1000B parameter model. It is really up to you. \n","\n","We don't really want you to do sloppy homework.\n","\n","The variable descriptions:\n","\n","- train set: kdd_train_probe\n","- validation set (for hyperparam tuning): kdd_valid_probe\n","- test set: kdd_test_v2_probe\n","\n","## What will you report?\n","Report your average macro f1 score on test set:\n","\n","```python\n","from sklearn.metrics import f1_score\n","f1 = f1_score(y_true, y_pred, average = \"macro\")\n","print(f1)\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"EIZsN_bUmDaD"},"source":["# Preparation (do not edit this part)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0foq7wEgm3hD"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import warnings\n","from pandas.core.common import SettingWithCopyWarning\n","\n","import torch.nn as nn\n","import torch\n","import sys\n","from torch.utils.data import DataLoader, Dataset\n","from collections import defaultdict\n","from tqdm.auto import tqdm\n","\n","import seaborn as sns\n","from pylab import rcParams\n","\n","from sklearn.metrics import f1_score, accuracy_score, classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHWVxbClBdhl"},"outputs":[],"source":["%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","\n","sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","\n","rcParams['figure.figsize'] = 10, 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjeIew14vgeQ"},"outputs":[],"source":["\n","warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n","\n","kdd = pd.read_csv('/content/drive/MyDrive/applied_ai_enes_safak/homework/kdd.csv')\n","kdd = kdd.iloc[:,1:43]\n","kdd = kdd.drop(['Protocol Type', 'Service', 'Flag'], axis = 1)\n","\n","kdd_train = kdd.iloc[0:102563, :]\n","kdd_test = kdd.iloc[102563:183737, :]\n","\n","kdd_train_probe = kdd_train[(kdd_train.Type_Groups == 'Normal') | (kdd_train.Type_Groups == 'Probe')]\n","kdd_test_probe = kdd_test[(kdd_test.Type_Groups == 'Normal') | (kdd_test.Type_Groups == 'Probe')]\n","\n","kdd_train_probe['Type_Groups'] = np.where(kdd_train_probe['Type_Groups'] == 'Normal', 0, 1)\n","kdd_test_probe['Type_Groups'] = np.where(kdd_test_probe['Type_Groups'] == 'Normal', 0, 1)\n","\n","kdd_valid_probe = kdd_test_probe.iloc[14000:34000,:]\n","kdd_test_v2_probe = pd.concat([kdd_test_probe.iloc[0:14000,:], kdd_test_probe.iloc[34001:64759,:]])\n","\n","\n","# classify anomalies and normals\n","# train set: kdd_train_probe\n","# validation set (for hyperparam tuning): kdd_valid_probe\n","# test set: kdd_test_v2_probe\n","# avg. macro f1 score on test set"]},{"cell_type":"markdown","metadata":{"id":"BcxDLPSSmIPd"},"source":["## Pytorch DataLoaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7bI7a2t2J4R"},"outputs":[],"source":["# create our dataloaders for train set and val set\n","# we will use autoencoders to detect anomalies, so we dont need anomaly class\n","# remove anomaly samples and train autoencoder to learn reconstruction of normal samples\n","\n","class TabularDataset(Dataset):\n","    def __init__(self, df):\n","        super(TabularDataset, self).__init__()\n","        self.df = df\n","    \n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        data = self.df.iloc[idx, :-1].to_numpy()\n","        return {\n","            \"sample\": torch.Tensor(data)\n","        }\n","\n","\n","BATCH_SIZE = 128\n","\n","train_normal = kdd_train_probe[kdd_train_probe.Type_Groups == 0]\n","val_normal = kdd_valid_probe[kdd_valid_probe.Type_Groups == 0]\n","test_normal = kdd_test_v2_probe[kdd_test_v2_probe.Type_Groups == 0]\n","\n","\n","train_data = TabularDataset(train_normal)\n","val_data = TabularDataset(val_normal)\n","test_data = TabularDataset(test_normal)\n","test_data_all = TabularDataset(kdd_test_v2_probe)\n","\n","train_dataloader = DataLoader(train_data, shuffle = True, batch_size = BATCH_SIZE) # for training\n","val_dataloader = DataLoader(val_data, shuffle = False, batch_size = BATCH_SIZE) # for validationg\n","test_dataloader = DataLoader(test_data, shuffle = False, batch_size = BATCH_SIZE) # for testing\n","test_all_dataloader = DataLoader(test_data_all, shuffle = False, batch_size = BATCH_SIZE) # contains both anomaly and normal samples to test"]},{"cell_type":"markdown","metadata":{"id":"ZPMxDRei_bpI"},"source":["# VAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6h9mMOzXwVdR"},"outputs":[],"source":["# VAE implementation in PyTorch\n","\n","class LinearVAE(nn.Module):\n","    def __init__(self, n_features, latent_dim):\n","        super(LinearVAE, self).__init__()\n","        self.n_features = n_features\n","\n","        self.encoder = nn.Sequential(\n","            nn.Linear(n_features, 20),\n","            nn.Tanh()\n","        )\n","\n","        self.encoder2mean = nn.Linear(20, latent_dim)\n","        self.encoder2logvar = nn.Linear(20, latent_dim)\n","\n","        self.decoder = nn.Sequential(\n","            nn.Linear(latent_dim, 20),\n","            nn.ReLU(),\n","            nn.Linear(20, n_features)\n","        )\n","    \n","    def forward(self, x):\n","        bs = x.size(0)\n","        out = self.encoder(x)\n","        mu = self.encoder2mean(out)\n","        log_var = self.encoder2logvar(out)\n","        z = self.reparameterize(mu, log_var)\n","        out = self.decoder(z)\n","        return out, mu, log_var\n","        \n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        sample = mu + (eps * std)\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Efh0YtnP1zSJ"},"outputs":[],"source":["def evaluate(...):\n","    # implement evaluating function over ```val_dataloader``` variable, to use in training function\n","    ...\n","\n","def train(...):\n","    # implement training function over ```train_dataloader``` variable\n","    ...\n","\n","def calculate_f1_score(...):\n","    # implement metric function to calculate macro f1 score over ```test_all_dataloader```\n","    # by using predefined threshold value\n","    # if overall loss > threshold, then it is anomaly; else normal\n","    ..."]},{"cell_type":"markdown","metadata":{"id":"oLgaxnaR_eEp"},"source":["# Vanilla AE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gwWry8U-3Zs"},"outputs":[],"source":["class VanillaAE(nn.Module):\n","    # implement vanilla autoencoder in PyTorch\n","    ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8VtgQtB_p8v"},"outputs":[],"source":["def evaluate(...):\n","    # implement evaluating function over ```val_dataloader``` variable, to use in training function\n","    ...\n","\n","def train(...):\n","    # implement training function over ```train_dataloader``` variable\n","    ...\n","\n","def calculate_f1_score(...):\n","    # implement metric function to calculate macro f1 score over ```test_all_dataloader```\n","    # by using predefined threshold value\n","    # if overall loss > threshold, then it is anomaly; else normal\n","    ..."]},{"cell_type":"markdown","metadata":{"id":"g0v3fAUHHlde"},"source":["# DAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Iyj39x6HmJq"},"outputs":[],"source":["class VanillaAE(nn.Module):\n","    # implement denoising autoencoder in PyTorch\n","    ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Btk1uZLvIOam"},"outputs":[],"source":["def evaluate(...):\n","    # implement evaluating function over ```val_dataloader``` variable, to use in training function\n","    ...\n","\n","def train(...):\n","    # implement training function over ```train_dataloader``` variable\n","    ...\n","\n","def calculate_f1_score(...):\n","    # implement metric function to calculate macro f1 score over ```test_all_dataloader```\n","    # by using predefined threshold value\n","    # if overall loss > threshold, then it is anomaly; else normal\n","    ..."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of Homework 1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}